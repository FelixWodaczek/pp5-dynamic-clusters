{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster Transitions for Dynamic by Design Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acquire Trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('./data').resolve()\n",
    "xtc_dir = data_dir.joinpath('Disordered_By_Design/XTC_files')\n",
    "\n",
    "unique_names = [name.stem for name in xtc_dir.glob('*.xtc')]\n",
    "\n",
    "print(f\"{len(unique_names)} unique starting configurations:\")\n",
    "print(unique_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import PDB\n",
    "\n",
    "pdb_dir = data_dir.joinpath('Disordered_By_Design/2KMV/')\n",
    "sample_file = pdb_dir.joinpath('2KMV_01_02.pdb')\n",
    "\n",
    "sample_pdb = PDB.PDBParser().get_structure('sample', sample_file)\n",
    "print(f\"Sample structure with {len([_ for atom in sample_pdb.get_atoms()])} models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mdtraj\n",
    "\n",
    "sample_u = mdtraj.load_xtc(\n",
    "    data_dir.joinpath('Disordered_By_Design/XTC_files/md_0_1_align_2KMV_01_02.xtc'),\n",
    "    top=data_dir.joinpath('Disordered_By_Design/2KMV/2KMV_01_02.pdb'),\n",
    "    atom_indices=range(0, 2834)\n",
    ")\n",
    "print(sample_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MDAnalysis import Universe\n",
    "\n",
    "sample_u = Universe(\n",
    "    topology=data_dir.joinpath('Disordered_By_Design/2KMV/2KMV_01_02.pdb'), topology_format='pdb',\n",
    "    trajectory=data_dir.joinpath('Disordered_By_Design/XTC_files/md_0_1_align_2KMV_01_02.xtc'), format='xtc'\n",
    ")\n",
    "\n",
    "for traj in sample_u.trajectory:\n",
    "    print(f\"Frame {traj.frame} has {len(sample_u.atoms)} atoms\")\n",
    "print(f\"Found {len(sample_u.atoms)} atoms\")\n",
    "print(f\"Found {len(sample_u.residues)} residues\")\n",
    "print(f\"Found {len(sample_u.segments)} segments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 2000\n",
    "import pickle\n",
    "with open(f'clusters-{K//1000}K.pkl', 'rb') as f:\n",
    "    clusters = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = clusters['X']\n",
    "C = np.stack(C)\n",
    "Cangles = np.array([clusters['phi0'], clusters['psi0'], clusters['phi1'], clusters['psi1']]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "neigh = NearestNeighbors().fit(C.reshape(C.shape[0], -1))\n",
    "#indices = neight.kneighbors(C.reshape(C.shape[0], -1), n_neighbors=1, return_distance=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coordinate getter function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio.PDB import Selection, Atom, Residue, Structure\n",
    "\n",
    "# data format seems to have changed, change alex' parser to give same results\n",
    "# # TODO: 'get_coordinates' function not really documented well enough to be sure, ask alex\n",
    "def get_coordinates(traj_path: Path, top_path: Path, filter_atoms=('N','CA','C','O')):\n",
    "    # load topology separately to get filter condition\n",
    "    topology: mdtraj.Topology = mdtraj.load_pdb(top_path).topology\n",
    "    \n",
    "    # we expect the atoms to be in the same order as filter_atoms\n",
    "    # it seems to always be the case for our pdb topologies, but this should somehow be checked\n",
    "    atom_order = {filter_atom: ii_atom for ii_atom, filter_atom in enumerate(filter_atoms)}\n",
    "    atom_filter = set(filter_atoms)\n",
    "\n",
    "    # get all residues and atoms that have all filter atoms in them\n",
    "    valid_atom_ids = []\n",
    "    valid_res_ids = []\n",
    "\n",
    "    # this should be a dictionary of residue index to residue name?\n",
    "    # is filtered afterwards for consecutive ids for some reason\n",
    "    valid_residue_names = {}\n",
    "    for residue in topology.residues:\n",
    "        # get all atoms in this residue\n",
    "        sub_indices = np.empty(len(filter_atoms), dtype=int)\n",
    "        valid_atom_counter = 0\n",
    "        for atom in residue.atoms:\n",
    "            if atom.name in atom_filter:\n",
    "                # count how many of filters are in residue\n",
    "                sub_indices[atom_order[atom.name]] = atom.index\n",
    "                valid_atom_counter += 1\n",
    "\n",
    "        if valid_atom_counter == len(filter_atoms):\n",
    "            # if all filter atoms are in residue, add to valid lists\n",
    "            valid_residue_names[residue.index] = str(residue)\n",
    "            valid_res_ids.append(residue.index)\n",
    "            valid_atom_ids.extend(list(sub_indices))\n",
    "\n",
    "    # only load atom ids according to filter condition\n",
    "    universe: mdtraj.Trajectory = mdtraj.load_xtc(traj_path, top=top_path, atom_indices=valid_atom_ids)\n",
    "\n",
    "    # extract coordinates as n_frames x n_residues x n_(filter_)atoms x 3\n",
    "    coords = universe.xyz.reshape((-1, len(valid_res_ids), len(filter_atoms), 3))\n",
    "    return coords, valid_residue_names\n",
    "\n",
    "sample_top = data_dir.joinpath(f\"Disordered_By_Design/2KMV/{unique_names[0].split('align_')[-1]}.pdb\")\n",
    "coords, valid_residue_names = get_coordinates(\n",
    "    traj_path=xtc_dir.joinpath(unique_names[0]+'.xtc'),\n",
    "    top_path=sample_top\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(valid_residue_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Canonicalize the coordinates and clusterization functionalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba\n",
    "\n",
    "@numba.njit(parallel=True)\n",
    "def canonize(X: np.ndarray):\n",
    "    #X = np.vstack(coords)\n",
    "    X = X - X[2,:]\n",
    "    \n",
    "    e1 = X[4,:]-X[2,:]\n",
    "    e1 = e1/np.linalg.norm(e1)\n",
    "    \n",
    "    e3 = np.cross(X[2,:]-X[4,:], X[5,:]-X[4,:])\n",
    "    e3 = e3/np.linalg.norm(e3)\n",
    "    \n",
    "    e2 = np.cross(e3, e1)\n",
    "    e2 = e2/np.linalg.norm(e2)\n",
    "    \n",
    "    U = np.zeros((3,3))\n",
    "    U[:,0] = e1\n",
    "    U[:,1] = e2\n",
    "    U[:,2] = e3    \n",
    "    #U = np.vstack([e1, e2, e3]).T\n",
    "\n",
    "    X = X @ U\n",
    "    return X\n",
    "    \n",
    "@numba.jit\n",
    "def clusterize(C, xcan):\n",
    "    d = np.sqrt(((xcan[None,:,:]-C)**2).sum(2).mean(1))\n",
    "    i = np.argmin(d)\n",
    "    return i, d[i]\n",
    "\n",
    "def clusterize_fast(neigh, xcan):\n",
    "    d, i = neigh.kneighbors(xcan.reshape(xcan.shape[0], -1), n_neighbors=1, return_distance=True)\n",
    "    return i[:,0], d[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def canonize_batch(X: np.ndarray):\n",
    "    # takes us from a ... x 2*n_atoms x 3 array to \"canonical descriptors\"\n",
    "    # these should actually be ... x 2*n_atoms*3 by simply collapsing at some point\n",
    "    # but for some reason this shape is kept until just before clustering\n",
    "    \n",
    "    #X = np.vstack(coords)\n",
    "\n",
    "    # center around C1\n",
    "    X = X - X[:,2,:][:,None,:]\n",
    "    \n",
    "    # angle N2-C1\n",
    "    e1 = X[:,4,:]-X[:,2,:]\n",
    "    e1 = e1/np.linalg.norm(e1, axis=1)[:,None]\n",
    "    \n",
    "    # something between C1-N2 and Ca2-N2\n",
    "    e3 = np.cross(X[:,2,:]-X[:,4,:], X[:,5,:]-X[:,4,:], axis=1)\n",
    "    e3 = e3/np.linalg.norm(e3, axis=1)[:,None]\n",
    "    \n",
    "    e2 = np.cross(e3, e1, axis=1)\n",
    "    e2 = e2/np.linalg.norm(e2, axis=1)[:,None]\n",
    "    \n",
    "    U = np.stack([e1, e2, e3], axis=2)\n",
    "    return np.einsum('nij,njk->nik', X, U) #X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusterize_traj(coords, residues):\n",
    "    # this only works because it sneakily pulls the cluster centers C from the global scope\n",
    "    import multiprocessing as mp\n",
    "    from process import canonize\n",
    "\n",
    "    inputs = []\n",
    "    indices = []\n",
    "    res_ids = list(residues.keys())\n",
    "    pairs = [res for res in range(coords.shape[1]-1) if res_ids[res+1] == res_ids[res]+1]\n",
    "    for frame in range(coords.shape[0]):\n",
    "        for pair, res in enumerate(pairs):\n",
    "            x = coords[frame,res:res+2,:]\n",
    "            inputs.append(x)\n",
    "            indices.append((frame, res, pair))\n",
    "\n",
    "    labels = np.zeros((coords.shape[0], len(pairs), 2))\n",
    "    with mp.Pool(mp.cpu_count(),) as pool:\n",
    "        for k, (xcan, (frame, res, pair)) in enumerate(zip(pool.imap(canonize, inputs, chunksize=100), indices)):\n",
    "            idx, d = clusterize(C, xcan)\n",
    "            labels[frame, pair, :] = [idx, d]\n",
    "    return labels, pairs\n",
    "\n",
    "\n",
    "def clusterize_traj_fast(coords, residues):\n",
    "    # this only works because it sneakily pulls the nearest neighbors object 'neigh' from the global scope\n",
    "    inputs = []\n",
    "\n",
    "    # indices contains frame, how manyth residue and pair number\n",
    "    indices = []\n",
    "    res_ids = list(residues.keys())\n",
    "    # find all residues with.. consecutive ids, will then be used as AD pairs\n",
    "    pairs = [res for res in range(coords.shape[1]-1) if res_ids[res+1] == res_ids[res]+1]\n",
    "\n",
    "    # this arranges the input data so that it can be canonized in parallel, will contain duplicates\n",
    "    # bit of a waste of memory, could be optimised, but for small number of pairs ok\n",
    "    # but should be: n_frames, n_ad_pairs, 8 (2*n_atoms), 3 (n_coordinates)\n",
    "    for frame in range(coords.shape[0]):\n",
    "        for pair, res in enumerate(pairs):\n",
    "            # get this and next residue\n",
    "            x = coords[frame, res:res+2, :]\n",
    "            # combine to AD pairs\n",
    "            inputs.append(np.vstack(x))\n",
    "            # collect frame, residue position in res_ids, and pair number\n",
    "            indices.append((frame, res, pair))\n",
    "\n",
    "    # now case to numpy array, this probably wouldn't be necessary if canonize_batch was properly done\n",
    "    inputs = np.stack(inputs, axis=0)\n",
    "    # this gives the 3 canonical coordinates for n_frames*n_pairs, n_atoms*2, 3\n",
    "    xcan = canonize_batch(inputs)\n",
    "    # the stacking seems redundant as there is nothing to stack\n",
    "    # maybe this is a mistake and this should be stacking the last two axes to create one n_atoms*2*3 descriptor?\n",
    "    # or left over from when this was done using lists\n",
    "    xcan = np.stack(xcan)\n",
    "\n",
    "    # clusterize_fast actually takes over the reshaping to make this canonical representation a descriptor\n",
    "    idxs, dists = clusterize_fast(neigh, xcan)\n",
    "    \n",
    "    # now reshape to original setup which is n_frames x n_pairs \n",
    "    # in each entry put nearest cluster center index and distance\n",
    "    labels = np.zeros((coords.shape[0], len(pairs), 2))\n",
    "    for i, d, (frame, res, pair) in zip(idxs, dists, indices):\n",
    "        labels[frame, pair, :] = [i, d]\n",
    "    \n",
    "    # return (cluster center index, distance), index of first residue in pair (next pair index is next residue)\n",
    "    return labels, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporal_rms(labels, C):\n",
    "    # if we assume that distances in C are meaningful\n",
    "    # now we get the variance of position in descriptor space is meaningful\n",
    "    # why we cannot just do autocorrelation of the distances is beyond me\n",
    "    rms = []\n",
    "    for pair in range(labels.shape[1]):\n",
    "        idx = [int(i) for i in sorted(set(labels[:,pair,0]))]\n",
    "        rms.append(np.sqrt(C[idx,:].reshape(len(idx), -1).var(0).sum()))\n",
    "    return np.array(rms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actually do for trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = {}\n",
    "for traj_path in xtc_dir.glob('*.xtc'):\n",
    "    traj_name = traj_path.stem\n",
    "    sample_top = data_dir.joinpath(f\"Disordered_By_Design/2KMV/{traj_name.split('align_')[-1]}.pdb\")\n",
    "    # by using the standard filter we get the backbone atoms\n",
    "    coords, residues = get_coordinates(traj_path, sample_top)\n",
    "    labels, pairs = clusterize_traj_fast(coords, residues)\n",
    "    rms = temporal_rms(labels, C)\n",
    "    results_dict[traj_name] = {'rms': rms, 'path': labels}\n",
    "    print(f\"{traj_path.stem}: {rms.mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No idea what this does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import linalg as la\n",
    "\n",
    "def hitting_times(T, max_iter = 10000):\n",
    "    one = np.ones(T.shape) - np.eye(T.shape[0])\n",
    "    k = np.zeros(T.shape[0])\n",
    "    for i in range(max_iter):\n",
    "        if i % 1000 == 0:\n",
    "            print(i)\n",
    "        k_old = k\n",
    "        k = one + T.dot(k)\n",
    "        k -= np.diag(np.diag(k))\n",
    "        if la.norm(k-k_old) < 1e-6:\n",
    "            break\n",
    "    return k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this calculates the transition matrix of going from one cluster to another\n",
    "# P[from_c, to_c]\n",
    "\n",
    "P = np.zeros((C.shape[0], C.shape[0]))\n",
    "for key, results in results_dict.items():\n",
    "    traj = results['path']\n",
    "    \n",
    "    for i, j in zip(traj[0:-1,:,0].reshape(-1), traj[1:,:,0].reshape(-1)):\n",
    "        P[int(i),int(j)] += 1/(traj.shape[0]-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_step = 0.01 # ns = 10ps\n",
    "n_from = P.sum(1)\n",
    "n_to = P.sum(0)\n",
    "idx_nonzero = (n_from>0) & (n_to > 0) # this should be | right?\n",
    "Pnrm = P[idx_nonzero,:][:,idx_nonzero] / n_from[idx_nonzero][:,None]\n",
    "Cnrm = C[idx_nonzero,:]\n",
    "Cangles_nrm = Cangles[idx_nonzero,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_lut = {}\n",
    "count = 0\n",
    "for l, idx in enumerate(idx_nonzero):\n",
    "    if idx:\n",
    "        count += 1\n",
    "        label_lut[l] = count-1\n",
    "    else:\n",
    "        label_lut[l] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels = np.array([label_lut[l] for l in clust_labels['labels']])\n",
    "idx_valid = (all_labels >= 0)\n",
    "all_labels = all_labels[idx_valid]\n",
    "all_angles = clust_labels['angles'][idx_valid,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.log(Pnrm[:, :]))\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = hitting_times(Pnrm, max_iter = 10000) * time_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 5\n",
    "plt.imshow(np.exp(-np.minimum(H,H.T)/t))\n",
    "plt.colorbar()\n",
    "#ticks = np.array(list(range(0,1000)))\n",
    "#plt.xticks(ticks, ticks)\n",
    "#plt.yticks(ticks, ticks)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pp5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
